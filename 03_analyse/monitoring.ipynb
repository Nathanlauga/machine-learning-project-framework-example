{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# transparentai package : https://github.com/Nathanlauga/transparentai\n",
    "from transparentai.datasets import load_adult, load_boston\n",
    "\n",
    "# transparentai.__SAVEPLOT__ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = {\n",
    "    'performance':{\n",
    "        'accuracy':0.0,\n",
    "        'f1':0.0\n",
    "    },\n",
    "    'bias':{\n",
    "        'protected_attr':{\n",
    "            \n",
    "        }\n",
    "        'dataset':{\n",
    "\n",
    "        },\n",
    "        'model':{\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bad_behavior = {\n",
    "    'same'\n",
    "}\n",
    "\n",
    "new_data = {\n",
    "    'X':None,\n",
    "    'y_pred':None,\n",
    "    'timestamp':None,\n",
    "    'y_real':None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'performance': {'accuracy': 0.0, 'f1': 0.0},\n",
       " 'bias': {'dataset': {}, 'model': {}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "\n",
    "I need to answer the following questions :\n",
    "- Is my model doing well since training ?\n",
    "- Are new data biased ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist new data : \n",
    "1. **New X ==> shape (m,n)** [MANDATORY]\n",
    "2. **predictions ==> shape (m,1)** [MANDATORY]\n",
    "3. [optional] y_real ==> shape (m,1) [RECOMANDED]\n",
    "5. Define protected_attr if not retrieve from training [RECOMANDED]\n",
    "6. [optional] Define bad_behavior for alert\n",
    "\n",
    "```\n",
    "[1] Dataset Bias (with/without comparison) [need protected_attr]\n",
    "[2a] Model Bias without y_real (with/without comparison) [need protected_attr]\n",
    "[2b] Model (with/without comparison) [need protected_attr]\n",
    "[3] perf (with/without comparison) \n",
    "[4] perf evolution \n",
    "```\n",
    "\n",
    "# I have originals metrics\n",
    "\n",
    "1. I don't have timestamp and y_real ==> [1][2a] with comparison \n",
    "2. I have only y_real ==> [1][2b][3] with comparison \n",
    "3. I have both ==> [1][2b][3][4] with comparison \n",
    "\n",
    "\n",
    "# I don't have originals metrics but original X_test & y_test\n",
    "\n",
    "Autogenerate training metrics (perf & bias)\n",
    "\n",
    "1. I don't have timestamp and y_real ==> [1][2a] with comparison \n",
    "2. I have only y_real ==> [1][2b][3] with comparison \n",
    "3. I have both ==> [1][2b][3][4] with comparison \n",
    "\n",
    "\n",
    "# I don't have originals metrics and not original X_test & y_test\n",
    "\n",
    "1. I don't have timestamp and y_real ==> [1][2a] without comparison \n",
    "2. I have only y_real ==> [1][2b][3] without comparison \n",
    "3. I have both ==> [1][2b][3][4] without comparison \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from transparentai.models import ClassificationModel, RegressionModel\n",
    "from transparentai.datasets import StructuredDataset\n",
    "from transparentai.fairness import DatasetBiasMetric, ModelBiasMetric\n",
    "\n",
    "class Monitoring():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y_preds, y_real=None, model_type='classification',\n",
    "                 orig_metrics=None, privileged_groups=None, alert_threshold=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if model_type not in ['classification','regression']:\n",
    "            raise ValueError('Only regression and classification are handled  for model_type.')\n",
    "        if type(X) not in [pd.DataFrame]:\n",
    "            raise TypeError('X has to be a pandas dataframe')\n",
    "        if len(X) != len(y_preds):\n",
    "            raise ValueError('y_preds and X must have the same length')\n",
    "        if y_real is not None:\n",
    "            if len(X) != len(y_real):\n",
    "                raise ValueError('y_real and X must have the same length')\n",
    "        if orig_metrics is not None:\n",
    "            metrics = ['performance','bias_dataset','bias_model']\n",
    "        \n",
    "        self.X = X\n",
    "        self.y_preds = y_preds\n",
    "        self.y_real = y_real\n",
    "        self.model_type = model_type\n",
    "        self.orig_metrics = orig_metrics\n",
    "        self.privileged_groups = privileged_groups\n",
    "        self.alert_threshold = alert_threshold\n",
    "        \n",
    "        df = X.copy()\n",
    "        df['target'] = y_preds if y_real is None else y_real\n",
    "        self.dataset = StructuredDataset(df=df, target='target')\n",
    "        \n",
    "        self._compute_new_metrics()\n",
    "        \n",
    "        if orig_metrics is not None:\n",
    "            self._check_orig_and_new_metrics()\n",
    "        \n",
    "        \n",
    "    def compute_orig_metrics(self, X_orig, y_orig):\n",
    "        \"\"\"\n",
    "        Only if you don't have original metrics already stored\n",
    "        \"\"\"\n",
    "        # Todo\n",
    "        orig_metrics = {}\n",
    "        \n",
    "        self.orig_metrics = orig_metrics\n",
    "        \n",
    "    def _compute_new_metrics(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        new_metrics = {}\n",
    "        # handle only 2 first ?\n",
    "#         model_bias_metrics = ['Disparate impact', 'Statistical parity difference']\n",
    "#         model_bias_metrics += ['Equal opportunity difference', 'Average abs odds difference', 'Theil index']\n",
    "        \n",
    "        # I have y_real ==> compute model perf & define model bias to compute\n",
    "        if (self.y_real is not None):\n",
    "            if self.model_type == 'classification':\n",
    "                model_obj = ClassificationModel\n",
    "            elif self.model_type == 'regression':\n",
    "                model_obj = RegressionModel\n",
    "                \n",
    "            model = model_obj(X=self.X, y=self.y_real, y_preds=y_preds)\n",
    "            new_metrics['performance'] = model.scores_to_json()\n",
    "            \n",
    "        \n",
    "        # I have protected attr ==> compute dataset bias & model bias\n",
    "        if (self.privileged_groups is not None):\n",
    "            \n",
    "            bias = DatasetBiasMetric(self.dataset, privileged_groups)\n",
    "            new_metrics['bias_dataset'] = bias.metrics_to_json()\n",
    "            \n",
    "            if self.y_real is not None:\n",
    "                bias = ModelBiasMetric(self.dataset, self.y_preds, self.privileged_groups)\n",
    "                new_metrics['bias_model'] = bias.metrics_to_json()\n",
    "            \n",
    "        self.new_metrics = new_metrics\n",
    "        \n",
    "    def _check_orig_and_new_metrics(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Check performance\n",
    "        \n",
    "        \n",
    "#         else:\n",
    "        # WARNING only n metrics can be compare\n",
    "        print()\n",
    "\n",
    "    def plot_perfomance(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.orig_metrics is None:\n",
    "            print()\n",
    "            # plot_bar_performance(self.new_metrics, alert_threshold=alert_threshold)\n",
    "        else:\n",
    "#             plot_bar_performance_comparison(self.new_metrics, \n",
    "#                                             self.orig_metrics,\n",
    "#                                             alert_threshold=alert_threshold)\n",
    "            print()\n",
    "    \n",
    "    def plot_bias(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.orig_metrics is None:\n",
    "            print()\n",
    "            # plot_gauge_bias(self.new_metrics, alert_threshold=alert_threshold)\n",
    "        else:\n",
    "#             plot_gauge_bias_comparison(self.new_metrics, \n",
    "#                                        self.orig_metrics,\n",
    "#                                        alert_threshold=alert_threshold)\n",
    "            print()\n",
    "\n",
    "    def insight(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        print()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'performance': {'MAE': 3.5300412243731283,\n",
       "  'MSE': 27.840428958079606,\n",
       "  'RMSE': 27.840428958079606,\n",
       "  'R2': 0.6077531114662875},\n",
       " 'bias_dataset': {'age category': {'<=22.68': '{\"Disparate impact\":3.6315789474,\"Statistical parity difference\":0.4385964912}',\n",
       "   '>22.68': '{\"Disparate impact\":0.4736842105,\"Statistical parity difference\":-0.4385964912}'}},\n",
       " 'bias_model': {'age category': {'<=22.68': '{\"Disparate impact\":2.1052631579,\"Statistical parity difference\":0.2763157895,\"Equal opportunity difference\":-0.2173913043,\"Average abs odds difference\":0.1253623188,\"Theil index\":0.1274653921}',\n",
       "   '>22.68': '{\"Disparate impact\":0.6315789474,\"Statistical parity difference\":-0.2763157895,\"Equal opportunity difference\":-0.0333333333,\"Average abs odds difference\":0.1253623188,\"Theil index\":0.0940768216}'}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from transparentai.datasets import load_adult, load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "boston['age category'] = np.where(boston['AGE'] < 26, 0,\n",
    "                                 np.where(boston['AGE'] < 61, 1, 2))\n",
    "target='MEDV'\n",
    "privileged_groups = {\n",
    "    'age category': [1]\n",
    "}          \n",
    "\n",
    "data = boston.copy()\n",
    "# data['age category'] = data['age category'].replace({'Young':0, 'Adult':1, 'Elder':2})\n",
    "\n",
    "X_reg, y_reg = data.drop(columns=target), data[target]\n",
    "reg = LinearRegression().fit(X_reg, y_reg)\n",
    "y_preds = reg.predict(X_reg)\n",
    "\n",
    "dataset = StructuredDataset(df=boston, target='MEDV')\n",
    "dataset_bias = DatasetBiasMetric(dataset=dataset, privileged_groups=privileged_groups)\n",
    "model = RegressionModel(model=reg)\n",
    "model.compute_scores(X=X_reg, y=y_reg)\n",
    "model_bias = ModelBiasMetric(dataset=dataset, preds=y_preds,\n",
    "                             privileged_groups=privileged_groups)\n",
    "\n",
    "metrics_bias_data = dataset_bias.metrics_to_json()\n",
    "metrics_bias_model = model_bias.metrics_to_json()\n",
    "metrics_performance = model.scores_to_json()\n",
    "\n",
    "orig_metrics = {\n",
    "    'performance':metrics_performance,\n",
    "    'bias_dataset':metrics_bias_data,\n",
    "    'bias_model':metrics_bias_model\n",
    "}\n",
    "\n",
    "new_X = X_reg.sample(50)\n",
    "new_y = y_reg.loc[new_X.index]\n",
    "y_preds = reg.predict(new_X)\n",
    "\n",
    "alert_threshold = {\n",
    "    'MAE':5.,\n",
    "    'MSE':50.,\n",
    "    'RMSE':45.,\n",
    "    'R2':0.6\n",
    "}\n",
    "\n",
    "test = Monitoring(X=new_X, y_preds=y_preds, y_real=new_y, privileged_groups=privileged_groups,\n",
    "                  model_type='regression', alert_threshold=alert_threshold)\n",
    "test.new_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'performance': {'accuracy': 0.84,\n",
       "  'f1': 0.84,\n",
       "  'precision': 0.84,\n",
       "  'recall': 0.84},\n",
       " 'bias_dataset': {'age category': {'<=50K': '{\"Disparate impact\":0.9807692308,\"Statistical parity difference\":-0.0147058824}',\n",
       "   '>50K': '{\"Disparate impact\":1.0625,\"Statistical parity difference\":0.0147058824}'},\n",
       "  'marital-status': {'<=50K': '{\"Disparate impact\":2.2983870968,\"Statistical parity difference\":0.5466893039}',\n",
       "   '>50K': '{\"Disparate impact\":0.0557184751,\"Statistical parity difference\":-0.5466893039}'},\n",
       "  'race': {'<=50K': '{\"Disparate impact\":1.0588235294,\"Statistical parity difference\":0.0444444444}',\n",
       "   '>50K': '{\"Disparate impact\":0.8181818182,\"Statistical parity difference\":-0.0444444444}'},\n",
       "  'gender': {'<=50K': '{\"Disparate impact\":1.4684210526,\"Statistical parity difference\":0.3022071307}',\n",
       "   '>50K': '{\"Disparate impact\":0.1483253589,\"Statistical parity difference\":-0.3022071307}'}},\n",
       " 'bias_model': {'age category': {'<=50K': '{\"Disparate impact\":1.105,\"Statistical parity difference\":0.0772058824,\"Equal opportunity difference\":0.1538461538,\"Average abs odds difference\":0.1394230769,\"Theil index\":0.1109035489}',\n",
       "   '>50K': '{\"Disparate impact\":0.7083333333,\"Statistical parity difference\":-0.0772058824,\"Equal opportunity difference\":0.125,\"Average abs odds difference\":0.1394230769,\"Theil index\":0.1109035489}'},\n",
       "  'marital-status': {'<=50K': '{\"Disparate impact\":2.7142857143,\"Statistical parity difference\":0.6315789474,\"Equal opportunity difference\":0.5,\"Average abs odds difference\":0.6136363636,\"Theil index\":0.1109035489}',\n",
       "   '>50K': '{\"Disparate impact\":0.0,\"Statistical parity difference\":-0.6315789474,\"Equal opportunity difference\":-0.7272727273,\"Average abs odds difference\":0.6136363636,\"Theil index\":0.1109035489}'},\n",
       "  'race': {'<=50K': '{\"Disparate impact\":1.3636363636,\"Statistical parity difference\":0.2666666667,\"Equal opportunity difference\":0.1176470588,\"Average abs odds difference\":0.422459893,\"Theil index\":0.1109035489}',\n",
       "   '>50K': '{\"Disparate impact\":0.0,\"Statistical parity difference\":-0.2666666667,\"Equal opportunity difference\":-0.7272727273,\"Average abs odds difference\":0.422459893,\"Theil index\":0.1109035489}'},\n",
       "  'gender': {'<=50K': '{\"Disparate impact\":1.6315789474,\"Statistical parity difference\":0.3870967742,\"Equal opportunity difference\":0.2,\"Average abs odds difference\":0.4636363636,\"Theil index\":0.1109035489}',\n",
       "   '>50K': '{\"Disparate impact\":0.0,\"Statistical parity difference\":-0.3870967742,\"Equal opportunity difference\":-0.7272727273,\"Average abs odds difference\":0.4636363636,\"Theil index\":0.1109035489}'}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transparentai.utils import encode_categorical_vars, labelencoder_to_dict\n",
    "\n",
    "adult = load_adult()\n",
    "adult['age category'] = np.where(adult['age'] < 26, 'Young',\n",
    "                                 np.where(adult['age'] < 61, 'Adult','Elder'))\n",
    "target='income'\n",
    "privileged_groups = {\n",
    "    'age category': ['Adult'],\n",
    "    'marital-status': ['Married-civ-spouse','Married-AF-spouse'],\n",
    "    'race': ['White'],\n",
    "    'gender': ['Male']\n",
    "}   \n",
    "\n",
    "target_value = {'>50K':1, '<=50K':0}\n",
    "adult[target] = adult[target].replace(target_value)\n",
    "adult, encoders = encode_categorical_vars(adult)\n",
    "X, y = adult.drop(columns=target), adult[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def decode_data(X):\n",
    "    df = X.copy()\n",
    "    for feature, encoder in encoders.items():\n",
    "        df[feature] = encoder.inverse_transform(df[feature])\n",
    "    return df \n",
    "\n",
    "X = decode_data(X_test)\n",
    "\n",
    "df = X.copy()\n",
    "target_value = {0:'<=50K', 1:'>50K'}\n",
    "df['income'] = y_test.replace(target_value)\n",
    "dataset = StructuredDataset(df=df,target='income')\n",
    "\n",
    "favorable_label = '>50K'\n",
    "dataset_bias = DatasetBiasMetric(dataset=dataset, privileged_groups=privileged_groups,\n",
    "                                 favorable_label=favorable_label)\n",
    "\n",
    "model = ClassificationModel(model=clf)\n",
    "model.compute_scores(X=X_test, y=y_test)\n",
    "\n",
    "preds = pd.Series(clf.predict(X_test)).replace(target_value)\n",
    "model_bias = ModelBiasMetric(dataset=dataset, preds=preds,\n",
    "                             privileged_groups=privileged_groups,\n",
    "                             favorable_label=favorable_label)\n",
    "\n",
    "metrics_bias_data = dataset_bias.metrics_to_json()\n",
    "metrics_bias_model = model_bias.metrics_to_json()\n",
    "metrics_performance = model.scores_to_json()\n",
    "\n",
    "orig_metrics = {\n",
    "    'performance':metrics_performance,\n",
    "    'bias_dataset':metrics_bias_data,\n",
    "    'bias_model':metrics_bias_model\n",
    "}\n",
    "\n",
    "\n",
    "sample = X_test.sample(50)\n",
    "X = decode_data(sample)\n",
    "\n",
    "new_X = X\n",
    "new_y = y_test.loc[new_X.index].replace(target_value)\n",
    "y_preds = pd.Series(clf.predict(sample)).replace(target_value)\n",
    "\n",
    "alert_threshold = {\n",
    "}\n",
    "\n",
    "test = Monitoring(X=new_X, y_preds=y_preds, y_real=new_y, privileged_groups=privileged_groups,\n",
    "                  model_type='classification')\n",
    "test.new_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'performance': {'accuracy': 0.864313190222112,\n",
       "  'f1': 0.8609861500652922,\n",
       "  'precision': 0.8597211166057749,\n",
       "  'recall': 0.864313190222112,\n",
       "  'roc_auc': [0.9104387547348203]},\n",
       " 'bias_dataset': {'age category': {'>50K': '{\"Disparate impact\":0.2513138549,\"Statistical parity difference\":-0.2218542129}'},\n",
       "  'marital-status': {'>50K': '{\"Disparate impact\":0.1419849266,\"Statistical parity difference\":-0.382217429}'},\n",
       "  'race': {'>50K': '{\"Disparate impact\":0.5474878574,\"Statistical parity difference\":-0.1143289004}'},\n",
       "  'gender': {'>50K': '{\"Disparate impact\":0.3544886573,\"Statistical parity difference\":-0.1933251261}'}},\n",
       " 'bias_model': {'age category': {'>50K': '{\"Disparate impact\":0.2399307681,\"Statistical parity difference\":-0.1926260559,\"Equal opportunity difference\":-0.0866979077,\"Average abs odds difference\":0.0770036441,\"Theil index\":0.1077434355}'},\n",
       "  'marital-status': {'>50K': '{\"Disparate impact\":0.0794715624,\"Statistical parity difference\":-0.373522116,\"Equal opportunity difference\":-0.2750159005,\"Average abs odds difference\":0.2269593328,\"Theil index\":0.1077434355}'},\n",
       "  'race': {'>50K': '{\"Disparate impact\":0.5162956959,\"Statistical parity difference\":-0.1046255856,\"Equal opportunity difference\":-0.0432015489,\"Average abs odds difference\":0.041002567,\"Theil index\":0.1077434355}'},\n",
       "  'gender': {'>50K': '{\"Disparate impact\":0.3021886109,\"Statistical parity difference\":-0.1820181164,\"Equal opportunity difference\":-0.1056775302,\"Average abs odds difference\":0.0876053944,\"Theil index\":0.1077434355}'}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0  2  3\n",
      "row_0      \n",
      "1      1  0\n",
      "2      2  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    2    3  1\n",
       "row_0             \n",
       "1      1.0  0.0  0\n",
       "2      2.0  1.0  0\n",
       "3      0.0  0.0  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([1,2,2,2])\n",
    "e = np.array([2,2,2,3])\n",
    "matrix = pd.crosstab(t, e)\n",
    "\n",
    "print(matrix)\n",
    "\n",
    "for val in matrix.columns.values:\n",
    "    if val not in matrix.index.values:\n",
    "        matrix.loc[val, :] = 0\n",
    "        \n",
    "for val in matrix.index.values:\n",
    "    if val not in matrix.columns.values:\n",
    "        matrix.loc[:, val] = 0\n",
    "        \n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
